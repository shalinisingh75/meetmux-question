{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f449b-ee50-4c22-803b-80b366231d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c40d43-e5e1-4eb7-975d-2003035c6e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75ae07c-460c-4fa3-aaad-38e5a7301f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers installed successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "print(\"Transformers installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cafa6b18-3682-4842-817c-f70ad1acc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e467f47-2f82-4489-8557-f68692f48a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb208738-d931-47db-a138-ab2abd1833e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764fa11b-4552-4b36-baef-1d682f0379cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Conversation ID', 'Timestamp', 'Sender', 'Message'], dtype='object')\n",
      "   Conversation ID         Timestamp  Sender  \\\n",
      "0                1  07-10-2025 10:15  User B   \n",
      "1                1  07-10-2025 10:15  User A   \n",
      "2                1  07-10-2025 10:16  User B   \n",
      "3                1  07-10-2025 10:16  User A   \n",
      "4                1  07-10-2025 10:17  User B   \n",
      "\n",
      "                                             Message  \n",
      "0  \"Hey, did you see the client's feedback on the...  \n",
      "1  \"Just saw it. They want a lot of changes to th...  \n",
      "2  \"Yeah, that's what I was thinking. It's a big ...  \n",
      "3  \"I'll start on the revisions. Can you update t...  \n",
      "4  \"Will do. I'll block out the rest of the week ...  \n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50630ec1-dab8-4d22-b976-78430edf9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data['Sender'] = data['Sender'].astype(str).str.strip()\n",
    "data['Message'] = data['Message'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae93055d-b236-4676-a83d-1b6844906c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversation pairs: 19\n"
     ]
    }
   ],
   "source": [
    "conversations = []\n",
    "for i in range(len(data) - 1):\n",
    "    if data['Sender'][i] != data['Sender'][i + 1]:  # ensure two-person chat\n",
    "        input_text = data['Message'][i]\n",
    "        output_text = data['Message'][i + 1]\n",
    "        conversations.append((input_text, output_text))\n",
    "\n",
    "print(\"Total conversation pairs:\", len(conversations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c5d980-0443-404e-8b57-557cbe5267c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770fef12ca3a454a94177b68d2879bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shali\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shali\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efbd45e4982479e9199ac8740ff1539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8462d19725034656892ede4dac789407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d46c2b5544ffb986b44fc38649044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc27c33ed9d4b5e8fea57389d5e6222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4418f0925b004dce9bbff426df608ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e7ecc7b7ed4b3296b34a79445255ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a30c27c-6007-4c63-9fd3-b4a2c8fa0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([pair[0] for pair in conversations],\n",
    "                   return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "labels = tokenizer([pair[1] for pair in conversations],\n",
    "                   return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cde95a54-d434-49d6-b363-ca6d698463d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ One training step completed! Loss: 9.824211120605469\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "outputs = model(**inputs, labels=labels[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"✅ One training step completed! Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "444cf434-e19e-4373-bdd1-3b2cab2c3470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "\n",
      "I'm not a big fan of the idea of a \"bigger\" or \"bigger\" than you. I'm a big fan of the idea of\n"
     ]
    }
   ],
   "source": [
    "def generate_reply(message):\n",
    "    input_ids = tokenizer.encode(message, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=40, pad_token_id=tokenizer.eos_token_id)\n",
    "    reply = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return reply\n",
    "\n",
    "print(generate_reply(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eeab92d-4f16-4014-9140-a6fda8d0e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as Model.joblib\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"Model.joblib\")\n",
    "print(\"✅ Model saved as Model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c932f7-e55b-4816-bc8c-c28d9c60a87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 9.50440384721771e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shali\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\shali\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\shali\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = conversations[0][1]\n",
    "generated = generate_reply(conversations[0][0])\n",
    "print(\"BLEU Score:\", sentence_bleu([reference.split()], generated.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a69ecd9-0075-4c89-bfd2-c01a36827636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: \"Hey, did you see the client's feedback on the mockups?\"\n",
      "\n",
      "\"I don't know, I don't know, I don't know, I don't know, I don\n",
      "Reference: \"Just saw it. They want a lot of changes to the color scheme.\"\n",
      "BLEU Score: 0.007913247271422612\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "reference = conversations[0][1]\n",
    "generated = generate_reply(conversations[0][0])\n",
    "print(\"Generated:\", generated)\n",
    "print(\"Reference:\", reference)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference.split()], generated.split(), smoothing_function=smooth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3b48169-93b8-4a67-b783-c573136d7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.5884\n",
      "Epoch 2, Loss: 7.0724\n",
      "Epoch 3, Loss: 6.3470\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):  # try 3 training epochs\n",
    "    outputs = model(**inputs, labels=labels[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc08d8b-98ce-47be-9b4a-75338b86bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you doing?\n",
      "\n",
      "I am not a good person.\n",
      "\n",
      "I am not a good person.\n",
      "Good morning, I was in the morning.\n",
      "\n",
      "\n",
      "\n",
      "I was in\n",
      "\n",
      "I was in\n",
      "\n",
      "I was in\n",
      "\n",
      "I was in\n",
      "\n",
      "I was in\n",
      "\n",
      "I was\n",
      "Tell me a joke.\n"
     ]
    }
   ],
   "source": [
    "print(generate_reply(\"What are you doing?\"))\n",
    "print(generate_reply(\"Good morning\"))\n",
    "print(generate_reply(\"Tell me a joke\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b1d33-55f9-4cd2-878b-f859a85098fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
